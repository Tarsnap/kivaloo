LBS-DynamoDB design
===================

The lbs-dynamodb block store is invoked as

# kivaloo-lbs-dynamodb -s <lbs socket> -t <dynamodb-kv socket>
      -b <item size> [-1] [-p <pidfile>]

It creates a socket <lbs socket> on which it listens for incoming connections,
accepts one at a time, and performs I/Os following the LBS protocol.  It
stores data in Amazon DynamoDB via the kivaloo-dynamodb-kv protocol, using
DynamoDB items of size <item size>; the LBS block size is slightly smaller.

The user is responsible for ensuring that the DynamoDB table at which the
dynamodb-kv daemon is pointed is initialized with a string partition key "K"
and no items.

The other options are:
  -p <pidfile>
	Write the daemon's process ID to the file <pidfile>.  Defaults to
	-p <lbs socket>.pid.  (Note that if <lbs socket> is not an absolute
	path, the default pid file location is in the current directory.)
  -1
	Exit after handling one connection.

NOTE: THE USER MUST ENSURE THAT THERE ARE NEVER TWO LBS-DYNAMODB-KV DAEMONS
USING THE SAME DYNAMODB TABLE.  Two daemons operating on the same table
will overwrite each other's data and generally cause utter chaos, and since
lbs-dynamodb must be able to resume after a crash, there is no way for
it to "lock" a bucket.

Overview
--------

LBS-DynamoDB implements an "append multiple blocks" / "read one block" /
"delete blocks up to" logging block store on top of Amazon DynamoDB, using
one DynamoDB item for each block.  Requests are made to DynamoDB via the
dynamodb-kv daemon; items are expressed as key-value pairs with a string
K=<64-bit ID in hex> and a binary V=<block data provided from upstream>.

Aside from the DynamoDB items which are block data -- which are written once
and never modified until they are deleted -- there are two other values which
are stored: The "next block" value and the "deleted to" value.  These act as
a head and tail: All blocks which have been written and have not been deleted
satisfy the condition "deleted to" <= block # < "next block".

The two "metadata" values are stored in a series of 4096 items with key
values K="m<3 hex digits>".  These are stored as a circular buffer with a
"generation number" added; on startup, a sequence of 13 reads are performed
executing a binary search to find the most recently stored values.  These
have a negligable cost (less than 0.015 cents/month of storage) and under
reasonable assumptions (metadata updates no more than once per 10 ms; at
least 10 provisioned writes per second per DynamoDB partition) this ensures
that the rewriting of items does not result in significant "hot spots".

Since we never reuse block IDs, we can read blocks using weak consistency,
and only perform a strongly consistent read in the (very unlikely) event that
the weakly consistent read does not find the block (because it hasn't been
propagated to all of the DynamoDB replicas yet).

Writes are performed by
1. Recording the new "next block",
2. Storing all of the blocks being appended *except the last one*, and then
3. Storing the last block.

This guarantees that if the final block has been stored then all previous
blocks have also been stored (even if requests are handled out of order); but
if we crash in the middle of the second phase the upstream service may see an
arbitrary subset of the non-final blocks as having been stored.  Since the
KVLDS daemon scans for the last page containing a B+Tree root node and skips
over any non-present pages, this works fine for KVLDS; other users may need
to have special handling of partial writes.

If KVLDS crashes and a new KVLDS connects, the new KVLDS will "see" the last
write performed by the deceased KVLDS iff the final block was written before
the new KVLDS looks for it; thus any acknowledged write is guaranteed to be
visible while unacknowledged writes offer no guarantee.

Code structure
--------------

main.c		-- Processes command line, connects to target, initializes
		   state, creates a listening socket, daemonizes, accepts
		   one connection at once, and runs the event loop.
dispatch.c	-- Reads requests from a connection and drops the connection
		   if one cannot be read.  Passes requests to state.c.
state.c		-- Manages the internal storage state and processes requests.
deleteto.c	-- Implements the DeleteTo algorithm.
objmap.c	-- Converts object #s into stringified keys.
